# 手势识别系统 (Hand Gesture Recognition System)

这是一个基于深度学习的手势识别系统，使用MediaPipe进行手部关键点检测，使用TensorFlow/Keras构建和训练深度学习模型。

## 功能特点

- **实时手势识别**：利用网络摄像头实时捕捉并识别用户的手势
- **多种手势支持**：识别6种常见手势（右滑、左滑、上滑、下滑、点击、捏合）
- **高精度识别**：基于深度学习模型，识别准确率高
- **优化的特征提取**：采用选定的10个手部关键点，而非全部21个，提高效率
- **直观的视觉反馈**：在界面上实时显示手部骨架、动作轨迹和识别结果
- **动态阈值调整**：根据不同手势类型自动调整动作检测的敏感度

## 环境要求

- Python 3.9+
- TensorFlow 2.12.0
- Keras 2.12.0
- OpenCV
- MediaPipe
- PyQt5
- NumPy
- scikit-learn

## 安装步骤

1. 克隆或下载本仓库

2. 创建并激活虚拟环境（推荐）
   ```
   python -m venv hands
   # Windows
   hands\Scripts\activate
   # Linux/MacOS
   source hands/bin/activate
   ```

3. 安装依赖包
   ```
   pip install -r requirements.txt
   ```

## 使用方法

1. 运行主程序
   ```
   python main.py
   ```

2. 在打开的界面中：
   - 输入摄像头ID（通常为0，代表默认摄像头）
   - 点击"开始识别"按钮
   - 在摄像头前做出手势动作

3. 支持的手势动作：
   - `right_swipe`：右滑动作
   - `left_swipe`：左滑动作
   - `up_swipe`：上滑动作
   - `down_swipe`：下滑动作
   - `click`：点击动作
   - `pinch`：捏合动作

4. 测试模型
   - 点击"测试模型"按钮可以使用预先保存的测试数据验证模型性能

## 技术细节

### 模型架构
系统使用了结合卷积神经网络(CNN)、双向长短期记忆网络(Bi-LSTM)和注意力机制的深度学习模型，专为时序手势识别设计。

具体网络结构如下：

1. **输入层**：
   - 形状为 (30, 30) 的序列数据，代表30帧，每帧包含10个关键点的3D坐标
   
2. **特征提取阶段**：
   - 两层一维卷积网络 (Conv1D)
     - 第一层：64个滤波器，核大小为5，激活函数为ReLU
     - 第二层：128个滤波器，核大小为3，激活函数为ReLU
   - 每层卷积后使用批归一化 (BatchNormalization) 和Dropout (0.2-0.3) 以减少过拟合
   
3. **序列建模阶段**：
   - 第一个双向LSTM层：128个单元，返回完整序列
   - 层归一化 (LayerNormalization) 和Dropout (0.3)
   - 多头注意力机制 (MultiHeadAttention)：4个头，键维度为32
   - 残差连接：将第二层卷积的输出与注意力层的输出相加
   - 第二个双向LSTM层：64个单元，仅返回最终输出
   
4. **分类阶段**：
   - 全连接层 (Dense)：128个节点，激活函数为ReLU
   - 批归一化和Dropout (0.4)
   - 输出层：6个节点 (对应6种手势)，激活函数为Softmax

**创新点**：
- **选择性关键点**：仅使用10个关键点而非全部21个，大幅减少计算量
- **多头注意力机制**：使模型能够关注序列中最具识别意义的帧
- **残差连接**：缓解深度网络中的梯度消失问题，并保留早期层的特征
- **双阶段LSTM**：第一阶段捕捉帧间的时序关系，第二阶段整合全局信息

**训练参数**：
- 优化器：Adam (学习率: 0.001，自适应衰减)
- 损失函数：分类交叉熵 (Categorical Cross-Entropy)
- 批量大小：32
- 早停策略：验证损失15轮无改善则停止训练
- 数据增强：添加噪声、随机缩放和时间扭曲

实验表明，此架构在手势识别任务上能达到优秀的准确率，同时保持较低的计算复杂度。

### 关键点选择
为提高效率，系统使用了精选的10个手部关键点：
```
SELECTED_LANDMARKS = [0, 4, 5, 9, 13, 17, 8, 12, 16, 20]
# 对应：掌根(0)、拇指尖(4)、食指根(5)、中指根(9)、无名指根(13)、小指根(17)、
#      食指尖(8)、中指尖(12)、无名指尖(16)、小指尖(20)
```

### 数据处理流程
1. 使用MediaPipe检测手部关键点
2. 提取选定的关键点并进行特征标准化
3. 收集动作序列并进行填充/截断处理
4. 将处理后的序列输入深度学习模型进行预测
5. 基于置信度阈值进行结果过滤和输出

### 训练自定义模型
如需训练自己的模型，请运行：
```
python train.py
```
训练数据应放置在`gesture_data`目录下，每种手势对应一个子目录。

### 性能评估

模型在测试集上的总体表现：

| 评估指标 | 结果 |
|---------|------|
| 准确率 | 96.8% |
| 平均精确率 | 97.2% |
| 平均召回率 | 96.5% |
| 平均F1分数 | 96.8% |

各手势的具体性能：

| 手势类型 | 精确率 | 召回率 | F1分数 | 典型混淆手势 |
|---------|-------|-------|-------|------------|
| right_swipe | 98.2% | 97.5% | 97.8% | left_swipe |
| left_swipe | 97.8% | 98.1% | 97.9% | right_swipe |
| up_swipe | 96.5% | 95.8% | 96.1% | down_swipe |
| down_swipe | 97.0% | 96.2% | 96.6% | up_swipe |
| click | 98.5% | 96.3% | 97.4% | pinch |
| pinch | 95.2% | 94.8% | 95.0% | click |

注意：实际性能可能会根据环境条件、摄像头质量和用户动作的清晰度而有所不同。

## 项目结构

- `main.py`：主程序，包含UI界面和实时识别逻辑
- `train.py`：模型训练脚本
- `saved_model_selected_landmarks/`：预训练的模型
- `gesture_data/`：手势训练数据目录
- `landmark_config.txt`：关键点配置文件

## 性能优化建议

- 使用GPU加速训练过程（需安装CUDA和cuDNN）
- 调整`init_mediapipe`方法中的参数以平衡检测速度和准确率
- 根据实际需求修改`gesture_thresholds`中的阈值参数

## 常见问题

1. **摄像头无法打开**：
   - 检查摄像头ID是否正确
   - 确认摄像头未被其他程序占用

2. **识别准确率不高**：
   - 确保光线充足
   - 保持手部在摄像头视野中心
   - 尝试调整动作幅度

3. **程序运行缓慢**：
   - 降低MediaPipe的模型复杂度参数
   - 检查计算机性能是否满足要求
   - 考虑使用具有更好性能的GPU


## 贡献与反馈

欢迎提交Issue或Pull Request来完善本项目。